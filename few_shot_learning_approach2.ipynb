{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f9f6b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from randimage import get_random_image, show_array\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_digits\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from keras import utils\n",
    "# from keras.utils.np_utils import to_categorical \n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Lambda, Dense, Dropout, Convolution2D, MaxPooling2D, Flatten,Activation\n",
    "from keras.models import Sequential, Model\n",
    "# from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.optimizers import RMSprop\n",
    "# from tensorflow.keras import optimizers\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt \n",
    "from keras import callbacks\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from keras.models import Model,load_model, model_from_json\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\n",
    "\n",
    "#enable eager execution in tensorflow\n",
    "tf.config.run_functions_eagerly(True)\n",
    "to_categorical = utils.to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3e4aa8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 22:37:20.828723: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-17 22:37:20.855435: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-17 22:37:20.856148: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-17 22:37:21.525501: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from randimage import get_random_image, show_array\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_digits\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tensorflow.keras import utils\n",
    "# from tensorflow.keras.utils.np_utils import to_categorical \n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, Lambda, Dense, Dropout, Convolution2D, MaxPooling2D, Flatten, Activation\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "# from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "# from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import optimizers\n",
    "import matplotlib.pyplot as plt \n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from tensorflow.keras.models import Model, load_model, model_from_json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\n",
    "\n",
    "# Enable eager execution in tensorflow\n",
    "tf.config.run_functions_eagerly(True)\n",
    "to_categorical = utils.to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6792097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_ = load_digits()\n",
    "target_ = digits_['target']\n",
    "target_ = train_labels = to_categorical(target_, num_classes=10)\n",
    "digits = digits_['data'].reshape(-1,8,8)\n",
    "\n",
    "digits_resize = np.zeros((len(digits),32,32))\n",
    "\n",
    "for x,y in enumerate(digits):\n",
    "    digits_resize[x] = (cv2.resize(y, dsize=(32,32), interpolation=cv2.INTER_CUBIC)+1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d077f789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 25, 25, 16)        1040      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 12, 12, 16)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 9, 9, 32)          8224      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 9, 9, 32)          0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2592)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 512)               1327616   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1470778 (5.61 MB)\n",
      "Trainable params: 1470778 (5.61 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      " 3/45 [=>............................] - ETA: 1s - loss: 3.0706 - accuracy: 0.1146"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emon/anaconda3/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:265: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 2s 44ms/step - loss: 1.5027 - accuracy: 0.4906 - val_loss: 0.5057 - val_accuracy: 0.8639\n",
      "Epoch 2/2\n",
      "45/45 [==============================] - 2s 42ms/step - loss: 0.5410 - accuracy: 0.8212 - val_loss: 0.3738 - val_accuracy: 0.8833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f1c5064ff10>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_base_network(input_shape=(32,32,1)):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(16, (8,8), strides=(1,1),activation=\"relu\", input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Convolution2D(32, (4,4),strides=(1,1), activation=\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation=\"relu\"))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(256, activation=\"relu\"))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(10,activation='softmax'))\n",
    "    return model\n",
    "\n",
    "model = build_base_network()\n",
    "rms = optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, amsgrad=False)#RMSprop()\n",
    "rms = RMSprop()\n",
    "earlyStopping = EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=3,\n",
    "                              verbose=1,\n",
    "                              restore_best_weights=True)\n",
    "callback_early_stop_reduceLROnPlateau=[earlyStopping]\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=rms,metrics=\"accuracy\")\n",
    "model.summary()\n",
    "\n",
    "batch_size=32\n",
    "model.fit(digits_resize.reshape(-1,32,32,1), target_, validation_split=.20,batch_size= batch_size, verbose=1, epochs=2, callbacks=callback_early_stop_reduceLROnPlateau)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bac25ee",
   "metadata": {},
   "source": [
    "# Generating dummy images for one shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5d82be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23205/4108693065.py:11: DeprecationWarning: non-integer arguments to randrange() have been deprecated since Python 3.10 and will be removed in a subsequent version\n",
      "  a,b = random.randrange(0,img_size[0]/4),random.randrange(0,img_size[0]/4)\n",
      "/tmp/ipykernel_23205/4108693065.py:12: DeprecationWarning: non-integer arguments to randrange() have been deprecated since Python 3.10 and will be removed in a subsequent version\n",
      "  c,d = random.randrange(img_size[0]/2,img_size[0]),random.randrange(img_size[0]/2,img_size[0])\n"
     ]
    }
   ],
   "source": [
    "one = []\n",
    "zero = []\n",
    "\n",
    "img_size = (32,32)\n",
    "\n",
    "for x in range(200):\n",
    "    \n",
    "    img = get_random_image(img_size)  \n",
    "    \n",
    "    #picking random a,b,c,d coordinates for plotting rectangle\n",
    "    a,b = random.randrange(0,img_size[0]/4),random.randrange(0,img_size[0]/4)\n",
    "    c,d = random.randrange(img_size[0]/2,img_size[0]),random.randrange(img_size[0]/2,img_size[0])\n",
    "    \n",
    "    value = random.sample([True,False],1)[0]\n",
    "    if value==False:\n",
    "        #plotting rectangle \n",
    "        img[a:c,b:d,0] = 25\n",
    "        img[a:c,b:d,1] = 25\n",
    "        img[a:c,b:d,2] = 25\n",
    "        #convert RGB image to black & white\n",
    "        img = np.asarray(Image.fromarray((img*255).astype(np.uint8)).convert('L'))/255\n",
    "        one.append(img)\n",
    "    else:\n",
    "        img = np.asarray(Image.fromarray((img*255).astype(np.uint8)).convert('L'))/255\n",
    "        zero.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fd2256b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    #euclidean distance, output for Siamese network\n",
    "    x, y = vects\n",
    "    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "def compute_accuracy(predictions, labels):\n",
    "    return labels[predictions.ravel() < 0.5].mean()\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    margin = 1\n",
    "    return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e209182e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50, 2, 1, 32, 32), (200, 2, 1, 32, 32), (50, 1), (200, 1))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_sample_size = 50\n",
    "test_sample_size = 200\n",
    "dim1,dim2 = 32,32\n",
    "count = 0\n",
    "\n",
    "x_pair = np.zeros([total_sample_size, 2, 1, dim1, dim2])\n",
    "y = np.zeros([total_sample_size,1])\n",
    "\n",
    "x_pair_test = np.zeros([test_sample_size, 2, 1, dim1, dim2])\n",
    "y_test = np.zeros([test_sample_size,1])\n",
    "\n",
    "for x in range(total_sample_size):\n",
    "    value = random.sample([True,False],1)[0]\n",
    "    if value:\n",
    "            pair = random.choices(one, k=2)\n",
    "            x_pair[x,0,0,:,:] = pair[0]\n",
    "            x_pair[x,1,0,:,:] = pair[1]\n",
    "            #setting label=1 for similar images\n",
    "            y[x] = 1\n",
    "    \n",
    "    else:\n",
    "            x_pair[x,0,0,:,:] = random.choices(one, k=1)[0]\n",
    "            x_pair[x,1,0,:,:] = random.choices(zero, k=1)[0]\n",
    "            #setting label=0 for dissimilar images\n",
    "            y[x] = 0\n",
    "\n",
    "\n",
    "for x in range(test_sample_size):\n",
    "    value = random.sample([True,False],1)[0]\n",
    "    if value:\n",
    "            pair = random.choices(one, k=2)\n",
    "            x_pair_test[x,0,0,:,:] = pair[0]\n",
    "            x_pair_test[x,1,0,:,:] = pair[1]\n",
    "            y_test[x] = 1\n",
    "    \n",
    "    else:\n",
    "            x_pair_test[x,0,0,:,:] = random.choices(one, k=1)[0]\n",
    "            x_pair_test[x,1,0,:,:] = random.choices(zero, k=1)[0]\n",
    "            y_test[x] = 0\n",
    "x_pair.shape, x_pair_test.shape, y.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb919dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the final output layer from the CNN we added for MNIST Classification\n",
    "model2= Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "\n",
    "input_dim = x_pair.shape[3:]+ tuple([1])\n",
    "\n",
    "img_a = Input(shape=input_dim)\n",
    "img_b = Input(shape=input_dim)\n",
    "\n",
    "feat_vecs_a = model2(img_a)\n",
    "feat_vecs_b = model2(img_b)\n",
    "# print(K.int_shape(feat_vecs_a))\n",
    "#Siamese output using utility functions declared above\n",
    "distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([feat_vecs_a, feat_vecs_b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13fd3d93",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 32, 32, 1)]          0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 32, 32, 1)]          0         []                            \n",
      "                                                                                                  \n",
      " model (Functional)          (None, 256)                  1468208   ['input_1[0][0]',             \n",
      "                                                                     'input_2[0][0]']             \n",
      "                                                                                                  \n",
      " lambda (Lambda)             (None, 1)                    0         ['model[0][0]',               \n",
      "                                                                     'model[1][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1468208 (5.60 MB)\n",
      "Trainable params: 1468208 (5.60 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rms = optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, amsgrad=False)#RMSprop()\n",
    "earlyStopping = EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=3,\n",
    "                              verbose=1,\n",
    "                              restore_best_weights=True)\n",
    "callback_early_stop_reduceLROnPlateau=[earlyStopping]\n",
    "\n",
    "model = Model(inputs=[img_a, img_b], outputs=distance)\n",
    "model.compile(loss=contrastive_loss, optimizer=rms,metrics=[accuracy])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0adea58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50, 32, 32, 1),\n",
       " (50, 32, 32, 1),\n",
       " (200, 32, 32, 1),\n",
       " (200, 32, 32, 1),\n",
       " (50, 1),\n",
       " (200, 1))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1 = x_pair[:, 0].reshape(-1,32,32,1)\n",
    "img2 = x_pair[:, 1].reshape(-1,32,32,1)\n",
    "\n",
    "img3 = x_pair_test[:, 0].reshape(-1,32,32,1)\n",
    "img4 = x_pair_test[:, 1].reshape(-1,32,32,1)\n",
    "img1.shape, img2.shape, img3.shape, img4.shape, y.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5ae597f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m img4 \u001b[38;5;241m=\u001b[39m x_pair_test[:, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      7\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[0;32m----> 8\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimg1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimg3\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimg4\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_early_stop_reduceLROnPlateau\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39msave_weights(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_weights.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_architecture.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:98\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m     97\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "img1 = x_pair[:, 0].reshape(-1,32,32,1)\n",
    "img2 = x_pair[:, 1].reshape(-1,32,32,1)\n",
    "\n",
    "img3 = x_pair_test[:, 0].reshape(-1,32,32,1)\n",
    "img4 = x_pair_test[:, 1].reshape(-1,32,32,1)\n",
    "\n",
    "batch_size = 8\n",
    "history = model.fit([img1, img2], y, validation_data=([img3,img4],y_test),\n",
    "      batch_size= batch_size, verbose=1, epochs=10, callbacks=callback_early_stop_reduceLROnPlateau)\n",
    "\n",
    "model.save_weights('model_weights.h5')\n",
    "with open('model_architecture.json', 'w') as f:\n",
    "    f.write(model.to_json())\n",
    "print('saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a5bf0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 1) (50, 1)\n"
     ]
    }
   ],
   "source": [
    "print(y_test.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61a8acb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wf_tr",
   "language": "python",
   "name": "wf_tr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
